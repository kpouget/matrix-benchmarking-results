Running non-interactively.
Using the current directory to store the test artifacts (/home/kevin/openshift/matrix_benchmark/results/mpi_benchmark/collective/aws-c4.xlarge/osu-allreduce_2x/run_220220124_1644.8f2b).
run ==> 2
cloud ==> aws
machine ==> c4.xlarge
network ==> SDN
env ==> prod
mode ==> collective
operation ==> osu-allreduce
node_count ==> 2
expe ==> collective

mpijob.kubeflow.org "osu-alltoall-16procs" deleted
pod "osu-alltoall-16procs-worker-0" deleted
pod "osu-alltoall-16procs-worker-1" deleted
pod "osu-alltoall-16procs-worker-10" deleted
pod "osu-alltoall-16procs-worker-11" deleted
pod "osu-alltoall-16procs-worker-12" deleted
pod "osu-alltoall-16procs-worker-13" deleted
pod "osu-alltoall-16procs-worker-14" deleted
pod "osu-alltoall-16procs-worker-2" deleted
Waiting for all the MPI Operator pods to disappear ...
Done.
go run apply_template.go -name osu-allreduce -machine c4.xlarge -np 2

Waiting for mpijob.kubeflow.org/osu-allreduce-2procs to complete its execution ...
....
Done, collecting artifacts in /home/kevin/openshift/matrix_benchmark/results/mpi_benchmark/collective/aws-c4.xlarge/osu-allreduce_2x/run_220220124_1644.8f2b ...

Failed to add the host to the list of known hosts (/root/.ssh/known_hosts).
Failed to add the host to the list of known hosts (/root/.ssh/known_hosts).

# OSU MPI Allreduce Latency Test v5.8
# Size       Avg Latency(us)
4                      64.09
8                      60.16
16                     79.44
32                     78.55
64                     87.49
128                    80.01
256                    87.25
512                    89.30
1024                   91.66
2048                   89.85
4096                  174.15
8192                  127.44
16384                 248.67
32768                 313.81
65536                 382.45
131072                976.76
262144               1423.12
524288               2295.21
1048576              4118.61
All done.
