2021-11-19 22:03:19.849128
Saving artifacts files into /home/kevin/openshift/matrix_benchmark/results/mlperf/dgx-test/ssd/20211119/gpu=full_1gpu_x_1pod/run20211119_20211119_220319_446225
Apply mixed MIG strategy ...
Node dgxa100 already labeled with MIG label 'all-disabled', nothing to do.
Deleting the old ConfigMap, if any ...
Existed.
Creating the new ConfigMap ...
Including my_run_and_time.sh ...
Deleting the old Job, if any ...
Deleting the old job Pods, if any ...
Done with the Pods.
Waiting for MIG reconfiguration of the node ...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
No nvidia.com/gpu resources...
MIG Manager state is success, good.
MIG label is all-disabled, good.
8 nvidia.com/gpu, good.
Launching 1 Pods ...
Running 1 Pods in parallel
Requesting 1 nvidia.com/gpu per Pod
Creating the new 'nvidia.com/gpu' Job ...

Job 'nvidia.com/gpu' for GPU: 1 x full x 1 Pods created!
=====
2021-11-19 22:06:32.758495
Waiting for run-ssd to terminate ...

run-ssd - Active (active=1, succeeded=0, failed=0)
[2Krun-ssd--1-x6bvp --> Pending
xxxxx[2K1 minutes of wait ...
xxxxxxxxxx[2K2 minutes of wait ...
xxxxxxxx[2Krun-ssd--1-x6bvp --> Running
Execution started!
............................................[2K5 minutes of execution ...
...............................................[2K10 minutes of execution ...
................................................[2K15 minutes of execution ...
...............................................[2K20 minutes of execution ...
.........................................[2K25 minutes of execution ...
...............................[2K30 minutes of execution ...
.........................[2K35 minutes of execution ...
.........................[2K40 minutes of execution ...
...................[2K45 minutes of execution ...
.....................................[2K50 minutes of execution ...
................................................[2K55 minutes of execution ...
...............................................[2K60 minutes of execution ...
................................................[2K65 minutes of execution ...
................................................[2K70 minutes of execution ...
...............................................[2K75 minutes of execution ...
.............................................
run-ssd - Finished (active=0, succeeded=1, failed=0)
-----
2021-11-19 23:28:15.242670
-----
Collecting artifacts ...
run-ssd--1-x6bvp --> Succeeded
/home/kevin/openshift/matrix_benchmark/results/mlperf/dgx-test/ssd/20211119/gpu=full_1gpu_x_1pod/run20211119_20211119_220319_446225/run-ssd--1-x6bvp.log
-----
2021-11-19 23:28:18.922121
Artifacts files saved into /home/kevin/openshift/matrix_benchmark/results/mlperf/dgx-test/ssd/20211119/gpu=full_1gpu_x_1pod/run20211119_20211119_220319_446225
